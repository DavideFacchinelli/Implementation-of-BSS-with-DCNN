{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementatio of \"Monoaural Audio Source Separation Using Deep Convolutional Neural Networks\"\n",
    "\n",
    "**Author**: Davide Facchinelli\n",
    " \n",
    "**Reference paper**: Chandna, P., Miron, M., Janer, J., and GÃ³mez, E. (2017). Monoaural Audio Source Separation Using Deep Convolutional Neural Networks. 13th International Conference on Latent Variable Analysis and Signal Separation (LVA ICA2017).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Other papers**:\n",
    "* Chandna, P. (2016). Audio Source Separation Using Deep Neural Networks, Master Thesis, Universitat Pompeu Fabra.\n",
    "* Vincent, E., Gribonval, R., and Fevotte, C. (2006). Performance measurement in blind audio source separation. IEEE Transactions on Audio, Speech and Language Processing, 14(4):1462-1469."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Part 1: Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The aim of the original work of Chandna et al. is to develop a deep convolutional neural network to separate a music track in its components. These components are the drums, the voice, the bass and all other sounds (instrumental or synthetic).\n",
    "\n",
    "The original article also considered a fifth track given by the combination of all the non-vocal source tracks. Here we choose to ignore it as non essential. It is an aid to predict the other four tracks, not an objective of the problem. The reason of this exclusion is to gain computational speed.\n",
    "\n",
    "Given a song they compute its Short-time Fourier transform with a 75% of overlapping, using a Hanning window of 1024 samples and a step size of 256 samples. Its magnitude and phase are given as input to the network. They divide the network in two stages: and encoding part and a decoding part. The encoding part is made by two convolutive layer and one dense layer. They then split the network in as many part as output tracks needed, and added an independent decoding part for each split. The decoding part is the inverse of the encoding: it is composed by a dense layer, and two deconvolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](OrNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center>Image taken from the reference paper.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Both the deconvolutions and convolutions do not use any activation function, instead both dense layers have a ReLu function as activation. The output of the network is then taken in module, this module also act as a non-linear component. Finally, the output of the network is used to mask the original magnitude. The inverse Short-time Fourier transform of the masked magnitude combined with the original phase give us the desired output track.\n",
    "\n",
    "The first convolutive layer is designed to capture the timbre feature of the song, where instead the second convolutive layer models the time-frequency characteristics of different instruments. Finally the dense layer achieve some dimensionality reduction and add non-linearity to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To train the network a peculiar loss function is used. The track that contains all the other instruments can be very variable, as in different song very different sound can be put in the \"other\" track. To solve this problem a loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$L = L_m + L_o$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "composed by two components is used.\n",
    "\n",
    "First let us define $y_o$ the \"other\" source track, $y_v$ the \"vocal\" source track, $y_b$ the \"bass\" source track and $y_d$ the \"drums\" source track. Their predictions are defined respectivly as $\\hat{y_o}$, $\\hat{y_v}$, $\\hat{y_b}$ and $\\hat{y_d}$\n",
    "\n",
    "The first component takes care of all track but the \"other\" one, considering their squared euclidean distance. It is also considered an additional term to penalize similarity between tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$L_m = ||y_d - \\hat{y_d}||^2+||y_b - \\hat{y_b}||^2+||y_v - \\hat{y_v}||^2 $$$$ - \\alpha(||\\hat{y_v} - \\hat{y_d}||^2+||\\hat{y_v} - \\hat{y_b}||^2+||\\hat{y_b} - \\hat{y_d}||^2 + ||\\hat{y_v} - \\hat{y_o}||^2 + ||\\hat{y_o} - \\hat{y_d}||^2 + ||\\hat{y_b} - \\hat{y_o}||^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The second component, that is always negative, enforce the distance between the true \"other\" track, and every other prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$L_o = -\\beta(||\\hat{y_b}-y_o||^2+||\\hat{y_d}-y_o||^2)-\\gamma||\\hat{y_v}-y_o||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We should remark how the true and predicted \"other\" track are never directly compared.\n",
    "\n",
    "$\\alpha, \\beta, \\gamma$ are weight to scale the penalizations and have been selected experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To evaluate the results three classical metrics used in sound signal separation are used:\n",
    "- SDR: source to distortion ratio\n",
    "- SIR: source to interference ratio\n",
    "- SAR: source to artifacts ratio\n",
    "\n",
    "In the original article also the source to noise ratio is considered, but we do not consider noise in our sets and therefore we ignore it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The dataset used is a public collection of 100 song of mixed genre provided with their four source tracks. The dataset can be found at https://sigsep.github.io/datasets/dsd100.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We trained the network with blocks of 10 songs at the time, with a 50% overlapping. For each block we trained the network for 10 epochs, with batches of 32 elements composed by 25 frames each shuffled across all the songs in the block. The procedure is repeated three times. We obtain the following training structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 3 repetitions\n",
    "  - 9 blocks, 50% overlapping\n",
    "    - 10 epochs\n",
    "      - 32 element batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the original article there were no division in blocks, all the 50 songs were considered at the same time and the net trained for 30 epochs. Our version of the training is a lot less demaning for the memory, as it needs to load in memory only 10 songs at the time.\n",
    "\n",
    "We used, as the original authors, the Adadelta algorithm to optimize our network. The parameters have been empirically choosed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Part 2: The code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Definition of global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Path for the dataset folder\n",
    "path = 'DSD100/'\n",
    "\n",
    "# Block of songs for the train\n",
    "blocks = [(0,10),(5,15),(10,20),(15,25),(20,30),(25,35),(30,40),(35,45),(40,50)]\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10\n",
    "repetitions = 3\n",
    "\n",
    "# Number of frame per input element\n",
    "T = 25\n",
    "\n",
    "# Channel number\n",
    "channels = 1\n",
    "\n",
    "# Number of element per batch\n",
    "batches = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Dataset building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We prepare the function to be called during the training. They will pass to the network the dataset build on the fly. In this way it's not necessary to import the song all at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def extractor(path,partial):\n",
    "    \"\"\"\n",
    "    Function to extract either the waveform or the stft transformation of a track.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        The path to the track to be imported.\n",
    "    partial : boolean\n",
    "        A boolean parameter to ask for either the waveform of the song (if True) or its stft transformation (if False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.Tensor\n",
    "        Contaning the waveform divided in sample if 'partial' is True, or its stft transformation if 'partial' is False.\n",
    "\n",
    "    tensorflow.Tensor\n",
    "        A zero-dimensional tensor contaning as number the sample rate of the wav decoding.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # We read the audio file\n",
    "    raw_audio = tf.io.read_file(path)\n",
    "    # Decode it in its waveform\n",
    "    audio, sample_rate = tf.audio.decode_wav(raw_audio, desired_channels=channels)\n",
    "    # Partitionate it in such a way that each elemnt will have a stft transformation of 25 frame\n",
    "    segmented = [audio[p:(p+256*(T+3)),:] for p in range(0,tf.shape(audio)[0],256*(T+3))]\n",
    "    # We 0 pad the tail to ensure that also the last part will have 25 frame\n",
    "    segmented[-1] = tf.pad(segmented[-1], [[0,256*(T+3) - tf.shape(segmented[-1])[0]],[0,0]])\n",
    "\n",
    "    if partial: return tf.stack(segmented), sample_rate\n",
    "    \n",
    "    # We compute the stft transformation\n",
    "    segmented = map(lambda segment: tf.signal.stft(tf.transpose(segment), frame_length=1024, frame_step=256, fft_length=1024),segmented)\n",
    "    segmented = list(map(lambda segment: tf.transpose(segment, perm = [1,2,0]),segmented))\n",
    "    \n",
    "    return tf.stack(segmented), sample_rate\n",
    "\n",
    "def mixturer(first, last, path):\n",
    "    \"\"\"\n",
    "    Function to build the input dataset X for the train.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first : int\n",
    "        Index of the first song to be considered.\n",
    "    last : int\n",
    "        Index of the last song to be considered.\n",
    "    path : str\n",
    "        Path to the folder where the dataset in contained.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.Tensor\n",
    "        Contaning the segmented stft transformation of the input song.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    tail = 'Mixtures/Dev'\n",
    "    \n",
    "    X = list()\n",
    "\n",
    "    # We build a tensor with all the segmented elements.\n",
    "    for song in listdir(path + tail)[first:last]:\n",
    "        X.append(extractor(path + tail + \"/\" + song + \"/mixture.wav\", False)[0])\n",
    "    X = tf.concat(X, 0)\n",
    "    \n",
    "    # We divide it in its magnitude and phase, and output them thogether\n",
    "    out = tf.stack([tf.abs(X), tf.math.angle(X)],4)\n",
    "    # We cut the last element in such a way that every batch will be of exactly 'batches' elements\n",
    "    return out[:len(out)-len(out)%batches]\n",
    "\n",
    "def sourcerer(first,last,path):\n",
    "    \"\"\"\n",
    "    Function to build the input dataset y_true for the train.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    first : int\n",
    "        Index of the first song to be considered.\n",
    "    last : int\n",
    "        Index of the last song to be considered.\n",
    "    path : str\n",
    "        Path to the folder where the dataset in contained.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.Tensor\n",
    "        Contaning the segmented stft transformation of the input sources.\n",
    "    \"\"\"\n",
    "    \n",
    "    tail = 'Sources/Dev'\n",
    "    \n",
    "    y_bass = list()\n",
    "    y_drums = list()\n",
    "    y_other = list()\n",
    "    y_vocals = list()\n",
    "\n",
    "    # We build a tensorflow tensor with all the segmented elements.\n",
    "    for song in listdir(path +  tail)[first:last]:\n",
    "        y_bass.append(extractor(path + tail + \"/\" + song + \"/bass.wav\", True)[0])\n",
    "        y_drums.append(extractor(path + tail + \"/\" + song + \"/drums.wav\", True)[0])\n",
    "        y_other.append(extractor(path + tail + \"/\" + song + \"/other.wav\", True)[0])\n",
    "        y_vocals.append(extractor(path + tail + \"/\" + song + \"/vocals.wav\", True)[0])\n",
    "\n",
    "    y_bass = tf.concat(y_bass, 0)\n",
    "    y_drums = tf.concat(y_drums, 0)\n",
    "    y_other = tf.concat(y_other, 0)\n",
    "    y_vocals = tf.concat(y_vocals, 0)\n",
    "    \n",
    "    out = tf.stack([y_bass, y_drums,y_other,y_vocals], 3)\n",
    "    # We cut the last element in such a way that every batch will be of exactly 'batches' elements\n",
    "    return out[:len(out)-len(out)%batches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We prepare our costum loss and optimizer. The metrics will be computed separatedly to don't slow down the traning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Classic Adadelta optimizer with costum parameters\n",
    "optimizer = tf.keras.optimizers.Adadelta(epsilon = 10e-1, learning_rate= 10e-3, rho = 0.95)\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Costum loss that suits our specific problem.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tensorflow.Tensor\n",
    "        The true sources.\n",
    "    y_pred : tensorflow.Tensor\n",
    "        The sources predicted by the network.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.Tensor\n",
    "        A zero-dimensional tensor contaning the total computed loss.\n",
    "    \"\"\"\n",
    "     \n",
    "    # We prepare a local function that compute the squared euclidean distance of a tensor, considering it as vecotrized\n",
    "    sqd = lambda v : tf.math.reduce_sum(tf.square(v))\n",
    "    \n",
    "    # We fix three weights\n",
    "    alpha = 0.001\n",
    "    beta = 0.01\n",
    "    gamma = 0.03\n",
    "    \n",
    "    # We divide our input in the 4 separate tracks\n",
    "    bass_pred = (y_pred[:,:,:,0])\n",
    "    drums_pred = (y_pred[:,:,:,1])\n",
    "    other_pred = (y_pred[:,:,:,2])\n",
    "    vocals_pred = (y_pred[:,:,:,3])\n",
    "    \n",
    "    bass_true = (y_true[:,:,:,0])\n",
    "    drums_true =(y_true[:,:,:,1])\n",
    "    other_true =(y_true[:,:,:,2])\n",
    "    vocals_true = (y_true[:,:,:,3])\n",
    "    \n",
    "    # We compute all the components of the loss\n",
    "    sq1 = sqd(bass_pred - bass_true) + sqd(drums_pred - drums_true) + sqd(vocals_pred - vocals_true)\n",
    "    diff = sqd(bass_pred - drums_pred) + sqd(bass_pred - vocals_pred) + sqd(vocals_pred - drums_pred)\n",
    "    diff_o = sqd(bass_pred - other_pred) + sqd(drums_pred - other_pred) + sqd(vocals_pred - other_pred)\n",
    "    diff = diff + diff_o\n",
    "    other = sqd(bass_pred - other_true) + sqd(drums_pred - other_true)\n",
    "    othervocals = sqd(vocals_pred - other_true)\n",
    "\n",
    "    # As we are considering all the batch thogether, we devide by their number before giving in output the value\n",
    "    return (sq1 - alpha * diff - beta * other - gamma * othervocals)/batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We prepare the function to build and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def conv(signal):\n",
    "    \"\"\"\n",
    "    Function that compute the inverse stft for each channel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : tensorflow.Tensor\n",
    "        A tensor contaning the transformed tracks.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.Tensor\n",
    "        A tensor contaning the waveform of the track given in input.\n",
    "    \"\"\"\n",
    "        \n",
    "    # We separate the track on the channels\n",
    "    signal = tf.unstack(signal,num=channels,axis=2)\n",
    "\n",
    "    out = list()\n",
    "\n",
    "    # Compute the inverse stft\n",
    "    for song in signal:\n",
    "        out.append(tf.signal.inverse_stft(song, frame_length=1024, frame_step=256, fft_length=1024,\n",
    "                                    window_fn=tf.signal.inverse_stft_window_fn(frame_step = 256)))\n",
    "    \n",
    "    # Rejoin the tracks in one multi-channel track\n",
    "    return tf.stack(out,1)\n",
    "\n",
    "def encoder(inp,t1,f1,N1,t2,f2,N2,N):\n",
    "    \"\"\"\n",
    "    Function that add the encoding block to our network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inp : tensorflow.keras.layers.Layer\n",
    "        The precedent layer.\n",
    "    t1 : int\n",
    "        First dimension of the first convlutional layer.\n",
    "    f1 : int\n",
    "        Second dimension of the first convlutional layer.\n",
    "    N1 : int\n",
    "        Number of filters of the first convolutional layer.\n",
    "    t2 : int\n",
    "        First dimension of the second convlutional layer.\n",
    "    f2 : int\n",
    "        Second dimension of the second convlutional layer.\n",
    "    N2 : int\n",
    "        Number of filters of the second convolutional layer.\n",
    "    N : int\n",
    "        Number of units in the encoding dense layer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.keras.layers.Layer\n",
    "        The resulting layer after the adding of this block.\n",
    "    \"\"\"\n",
    "    x = tf.keras.layers.Conv2D(filters = N1, kernel_size=(t1,f1), name = 'vertical_convolution')(inp)\n",
    "    x = tf.keras.layers.Conv2D(filters = N2, kernel_size=(t2,f2), name = 'horizontal_convolution')(x)\n",
    "    x = tf.keras.layers.Flatten(name = 'flattening')(x)\n",
    "    return tf.keras.layers.Dense(N, activation='relu', name = 'global_dense')(x)\n",
    "\n",
    "def decoder(x, t1, f1, N1,t2,f2,N2, track):\n",
    "    \"\"\"\n",
    "    Function that add the decoding block to our network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tensorflow.keras.layers.Layer\n",
    "        The precedent layer.\n",
    "    t1 : int\n",
    "        First dimension of the first convlutional layer.\n",
    "    f1 : int\n",
    "        Second dimension of the first convlutional layer.\n",
    "    N1 : int\n",
    "        Number of filters of the first convolutional layer.\n",
    "    t2 : int\n",
    "        First dimension of the second convlutional layer.\n",
    "    f2 : int\n",
    "        Second dimension of the second convlutional layer.\n",
    "    N2 : int\n",
    "        Number of filters of the second convolutional layer.\n",
    "    track : str\n",
    "        The name to have associated with the different step of this block.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.keras.layers.Layer\n",
    "        The resulting layer after the adding of this block.\n",
    "    \"\"\"\n",
    "    y = tf.keras.layers.Dense((T-t2+1)*N2, activation='relu', name = 'single_dense_' + track)(x)\n",
    "    y = tf.keras.layers.Reshape(((T-t2+1),1,N2), name = 'reshape_'+track)(y)\n",
    "    y = tf.keras.layers.Conv2DTranspose(filters = N1, kernel_size = (t2,f2), name = 'horizontal_deconvolution_' + track)(y)\n",
    "    y = tf.keras.layers.Conv2DTranspose(filters=channels, kernel_size=(t1,f1), name = 'vertical_deconvolution_' + track)(y)\n",
    "    y = tf.keras.layers.Lambda(tf.abs, name = 'module_' + track)(y)\n",
    "    return y\n",
    "\n",
    "def masker_converter(y, tot,magnitude, name):\n",
    "    \"\"\"\n",
    "    Function to apply the mask to the original magnitude.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : tensorflow.keras.layers.Layer\n",
    "        The precedent layer.\n",
    "    tot : tensorflow.keras.layers.Layer\n",
    "        Layer where the total value of the sum of all the computed magnitudes.\n",
    "    magnitude : tensorflow.keras.layers.Layer\n",
    "        Original magnitude given in input to be masked.\n",
    "    name : str\n",
    "        The name to have associated with the different step of this block.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.keras.layers.Layer\n",
    "        The resulting layer after the adding of this block.\n",
    "    \"\"\"\n",
    "    y = tf.keras.layers.Lambda(lambda c : tf.math.divide(c[0],c[1]), name='division_'+name)([y,tot])\n",
    "    y = tf.keras.layers.Multiply(name='y_'+name)([magnitude,y])\n",
    "    return y\n",
    "\n",
    "def complexer(z, phase, track):\n",
    "    \"\"\"\n",
    "    Function to put thogether the phase and magnitude and obtain the estimated stft transformation of our final track.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : tensorflow.keras.layers.Layer\n",
    "        The precedent layer.\n",
    "    phase : tensorflow.keras.layers.Layer\n",
    "        Original phase given in input.\n",
    "    track : str\n",
    "        The name to have associated with the different step of this block.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.keras.layers.Layer\n",
    "        The resulting layer after the adding of this block.\n",
    "    \"\"\"\n",
    "    z = tf.keras.layers.Lambda(lambda s:tf.math.multiply(tf.complex(s[0],.0), tf.math.exp(tf.complex(.0,s[1]))), name = 'complex_'+track)([z,phase])\n",
    "    return tf.keras.layers.Lambda(lambda v:tf.stack(list(map(conv,tf.unstack(v,num=batches,axis=0)))), name = 'istft_'+track)(z)\n",
    "    \n",
    "def buildier(t1,f1,N1,t2,f2,N2,N):\n",
    "    \"\"\"\n",
    "    Function that build and compile the whole model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t1 : int\n",
    "        First dimension of the first convlutional layer.\n",
    "    f1 : int\n",
    "        Second dimension of the first convlutional layer.\n",
    "    N1 : int\n",
    "        Number of filters of the first convolutional layer.\n",
    "    t2 : int\n",
    "        First dimension of the second convlutional layer.\n",
    "    f2 : int\n",
    "        Second dimension of the second convlutional layer.\n",
    "    N2 : int\n",
    "        Number of filters of the second convolutional layer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.keras.layers.Layer\n",
    "        The resulting layer after the adding of this block.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We take as input the stft transformation\n",
    "    stft = tf.keras.layers.Input(shape = (T, 513, channels, 2), name='stft', batch_size=batches)\n",
    "    \n",
    "    # Divide it in magnitude and phase\n",
    "    magnitude = tf.keras.layers.Lambda(lambda s:tf.unstack(s,num=2,axis = -1)[0], name = 'magnitude')(stft)\n",
    "    phase = tf.keras.layers.Lambda(lambda s:tf.unstack(s,num=2,axis = -1)[1], name = 'phase')(stft)\n",
    "    \n",
    "    # Apply the encoding block\n",
    "    x = encoder(magnitude,t1,f1,N1,t2,f2,N2,N)\n",
    "    \n",
    "    # Split in four tracks and apply the decoding blocks\n",
    "    bass = decoder(x, t1, f1, N1,t2,f2,N2, 'bass')\n",
    "    drums = decoder(x, t1, f1, N1,t2,f2,N2, 'drums')\n",
    "    other = decoder(x, t1, f1, N1,t2,f2,N2, 'other')\n",
    "    vocals = decoder(x, t1, f1, N1,t2,f2,N2, 'vocals')\n",
    "    \n",
    "    # Add the four estimated elements\n",
    "    added = tf.keras.layers.Add(name = 'sum')([bass,drums,other,vocals])\n",
    "\n",
    "    # Apply the mask to each track\n",
    "    bass = masker_converter(bass, added, magnitude, name = 'bass')\n",
    "    drums = masker_converter(drums, added, magnitude, name = 'drums')\n",
    "    other = masker_converter(other, added, magnitude, name = 'other')\n",
    "    vocals = masker_converter(vocals, added, magnitude, name = 'vocals')\n",
    "    \n",
    "    # Get the whole stft of each track\n",
    "    bass = complexer(bass, phase, track = 'bass')\n",
    "    drums = complexer(drums, phase, track = 'drums')\n",
    "    other = complexer(other, phase, track = 'other')\n",
    "    vocals = complexer(vocals, phase, track = 'vocals')\n",
    "    \n",
    "    out = tf.keras.layers.Lambda(lambda x:tf.keras.backend.stack(x,3), name = 'output_stacking')([bass,drums,other,vocals])\n",
    "    \n",
    "    # Initialize and compile the model\n",
    "    model = tf.keras.Model(inputs = stft, outputs = out)\n",
    "    model.compile(loss = loss, optimizer = optimizer, experimental_run_tf_function=False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We call the precedent defined functions, and provide the code to train the net. We also include a file with the weights of the already trained network, as the training may take a lot of time depending on the machine used.\n",
    "\n",
    "We also specify the number of units used in the different layers, using the same as in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model: \"model\"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Layer (type)                    Output Shape         Param #     Connected to                     '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================================================================================================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'stft (InputLayer)               [(32, 25, 513, 1, 2) 0                                            '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'magnitude (Lambda)              (32, 25, 513, 1)     0           stft[0][0]                       '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'vertical_convolution (Conv2D)   (32, 25, 1, 50)      25700       magnitude[0][0]                  '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'horizontal_convolution (Conv2D) (32, 14, 1, 30)      18030       vertical_convolution[0][0]       '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'flattening (Flatten)            (32, 420)            0           horizontal_convolution[0][0]     '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'global_dense (Dense)            (32, 128)            53888       flattening[0][0]                 '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'single_dense_bass (Dense)       (32, 420)            54180       global_dense[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'single_dense_drums (Dense)      (32, 420)            54180       global_dense[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'single_dense_other (Dense)      (32, 420)            54180       global_dense[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'single_dense_vocals (Dense)     (32, 420)            54180       global_dense[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'reshape_bass (Reshape)          (32, 14, 1, 30)      0           single_dense_bass[0][0]          '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'reshape_drums (Reshape)         (32, 14, 1, 30)      0           single_dense_drums[0][0]         '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'reshape_other (Reshape)         (32, 14, 1, 30)      0           single_dense_other[0][0]         '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'reshape_vocals (Reshape)        (32, 14, 1, 30)      0           single_dense_vocals[0][0]        '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'horizontal_deconvolution_bass ( (32, 25, 1, 50)      18050       reshape_bass[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'horizontal_deconvolution_drums  (32, 25, 1, 50)      18050       reshape_drums[0][0]              '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'horizontal_deconvolution_other  (32, 25, 1, 50)      18050       reshape_other[0][0]              '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'horizontal_deconvolution_vocals (32, 25, 1, 50)      18050       reshape_vocals[0][0]             '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'vertical_deconvolution_bass (Co (32, 25, 513, 1)     25651       horizontal_deconvolution_bass[0]['"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'vertical_deconvolution_drums (C (32, 25, 513, 1)     25651       horizontal_deconvolution_drums[0]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'vertical_deconvolution_other (C (32, 25, 513, 1)     25651       horizontal_deconvolution_other[0]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'vertical_deconvolution_vocals ( (32, 25, 513, 1)     25651       horizontal_deconvolution_vocals[0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'module_bass (Lambda)            (32, 25, 513, 1)     0           vertical_deconvolution_bass[0][0]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'module_drums (Lambda)           (32, 25, 513, 1)     0           vertical_deconvolution_drums[0][0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'module_other (Lambda)           (32, 25, 513, 1)     0           vertical_deconvolution_other[0][0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'module_vocals (Lambda)          (32, 25, 513, 1)     0           vertical_deconvolution_vocals[0]['"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'sum (Add)                       (32, 25, 513, 1)     0           module_bass[0][0]                '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 module_drums[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 module_other[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 module_vocals[0][0]              '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'division_bass (Lambda)          (32, 25, 513, 1)     0           module_bass[0][0]                '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 sum[0][0]                        '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'division_drums (Lambda)         (32, 25, 513, 1)     0           module_drums[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 sum[0][0]                        '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'division_other (Lambda)         (32, 25, 513, 1)     0           module_other[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 sum[0][0]                        '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'division_vocals (Lambda)        (32, 25, 513, 1)     0           module_vocals[0][0]              '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 sum[0][0]                        '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_bass (Multiply)               (32, 25, 513, 1)     0           magnitude[0][0]                  '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 division_bass[0][0]              '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'phase (Lambda)                  (32, 25, 513, 1)     0           stft[0][0]                       '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_drums (Multiply)              (32, 25, 513, 1)     0           magnitude[0][0]                  '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 division_drums[0][0]             '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_other (Multiply)              (32, 25, 513, 1)     0           magnitude[0][0]                  '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 division_other[0][0]             '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'y_vocals (Multiply)             (32, 25, 513, 1)     0           magnitude[0][0]                  '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 division_vocals[0][0]            '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'complex_bass (Lambda)           (32, 25, 513, 1)     0           y_bass[0][0]                     '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 phase[0][0]                      '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'complex_drums (Lambda)          (32, 25, 513, 1)     0           y_drums[0][0]                    '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 phase[0][0]                      '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'complex_other (Lambda)          (32, 25, 513, 1)     0           y_other[0][0]                    '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 phase[0][0]                      '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'complex_vocals (Lambda)         (32, 25, 513, 1)     0           y_vocals[0][0]                   '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 phase[0][0]                      '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'istft_bass (Lambda)             (32, 7168, 1)        0           complex_bass[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'istft_drums (Lambda)            (32, 7168, 1)        0           complex_drums[0][0]              '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'istft_other (Lambda)            (32, 7168, 1)        0           complex_other[0][0]              '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'istft_vocals (Lambda)           (32, 7168, 1)        0           complex_vocals[0][0]             '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'output_stacking (Lambda)        (32, 7168, 1, 4)     0           istft_bass[0][0]                 '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 istft_drums[0][0]                '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 istft_other[0][0]                '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'                                                                 istft_vocals[0][0]               '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=================================================================================================='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Total params: 489,142'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Trainable params: 489,142'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Non-trainable params: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'__________________________________________________________________________________________________'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = buildier(t1 = 1,f1 = 513,N1 = 50,t2 = 12,f2 = 1,N2 = 30, N = 128)\n",
    "net.summary(print_fn=display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if True: net.load_weights('pretrained_weights_experiment1.h5')\n",
    "else:\n",
    "    for _ in range(repetitions):\n",
    "        for block in blocks:\n",
    "            net.fit(x = mixturer(*block, path),\n",
    "                    y = sourcerer(*block, path),\n",
    "                    epochs = epochs, batch_size=batches, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here we provide the code we used to test the network, and to save the predicted audio track in .wav files to listen to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def tracks_predict(path, net):\n",
    "    \"\"\"\n",
    "    Function that, given the trained network, predict the four source track that should compose it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to reach the song to be separeted.\n",
    "    net : tensorflow.keras.Model\n",
    "        The trained model to be used to separate our track.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.keras.layers.Layer\n",
    "        A tensor contaning the 4 predicte source.\n",
    "    tensorflow.Tensor\n",
    "        A zero-dimensional tensor contaning as number the sample rate of the wav decoding.\n",
    "    \"\"\"\n",
    "    # We exctract the waveform from the file\n",
    "    stft, sample_rate = extractor(path, False)\n",
    "    # We shape is as our net expect to get it\n",
    "    stft = [stft[i:32+i] for i in range(0,int(tf.shape(stft)[0])-31,32)]\n",
    "    stft = list(map(lambda v:tf.stack([tf.abs(v), tf.math.angle(v)],4), stft))\n",
    "    # We compute the prediction\n",
    "    prediction = list(map(lambda v:net.predict(v),stft))\n",
    "    # We reshapre the output as a normal track waveform\n",
    "    prediction = [inner for outer in list(map(lambda v:tf.unstack(v,axis=0),prediction)) for inner in outer]\n",
    "    prediction = tf.concat(prediction,0)\n",
    "    \n",
    "    return prediction, sample_rate\n",
    "\n",
    "def measurer(y_true_m,y_pred_m):\n",
    "    \"\"\"\n",
    "    Function that compute SDR, SIR and SAR.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true_m : tensorflow.Tensor\n",
    "        All the true sources stacked.\n",
    "    y_pred_m : tensorflow.Tensor\n",
    "        All the predicted sources stacked.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tensorflow.Tensor objects, one for each channel, contaning the metrics' result.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We cut the tail of the true song to match the one of the predicted one\n",
    "    y_true_m = y_true_m[:int(tf.shape(y_pred_m)[0]),:,:]\n",
    "    \n",
    "    out = list()\n",
    "    \n",
    "    # We iterate over the channels\n",
    "    for i in range(channels):\n",
    "        y_true = tf.transpose(y_true_m[:,i,:])\n",
    "        y_pred = tf.transpose(y_pred_m[:,i,:])\n",
    "        \n",
    "        # We compute the target vector\n",
    "        s_target = tf.stack([y_true[i] * tf.tensordot(y_true[i], y_pred[i],1)/tf.tensordot(y_true[i], y_true[i],1) for i in range(4)])\n",
    "        \n",
    "        # We compute the projection of the predicted vector onto the space of the true sources\n",
    "        R_ss = tf.tensordot(y_true,tf.transpose(y_true),1)\n",
    "        R_ss = tf.linalg.inv(R_ss)\n",
    "        c = tf.stack([tf.tensordot(R_ss, tf.tensordot(y_true, y_pred[i], 1), 1) for i in range(4)])\n",
    "        P_s = tf.tensordot(c,y_true,1)\n",
    "        \n",
    "        # We compute the interference vector\n",
    "        e_interf = P_s - s_target\n",
    "\n",
    "        # We compute the artifacts vector\n",
    "        e_artif = y_pred - P_s\n",
    "\n",
    "        # We compute the SDR,\n",
    "        t = e_interf + e_artif\n",
    "        SDR = [4.342944819 * tf.math.log(tf.tensordot(s_target[i],s_target[i],1)/tf.tensordot(t[i],t[i],1)) for i in range(4)]\n",
    "        # the SIR,\n",
    "        SIR = [4.342944819 * tf.math.log(tf.tensordot(s_target[i],s_target[i],1)/tf.tensordot(e_interf[i],e_interf[i],1)) for i in range(4)]\n",
    "\n",
    "        # and the SAR\n",
    "        t = s_target + e_interf\n",
    "        SAR = [4.342944819 * tf.math.log(tf.tensordot(t[i],t[i],1)/tf.tensordot(e_artif[i],e_artif[i],1)) for i in range(4)]\n",
    "\n",
    "        out.append(tf.stack([SDR, SIR, SAR]))\n",
    "    return out\n",
    "\n",
    "def comparer(n):    \n",
    "    \"\"\"\n",
    "    Function that call measurer for all the songs and compute mean and STD of the results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        The number of songs to run the test on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensorflow.Tensor\n",
    "        A 2D tensor having as rows the metrics SDR, SIR and SAR means, and as columns the sources.\n",
    "    tensorflow.Tensor\n",
    "        A 2D tensor having as rows the metrics SDR, SIR and SAR std, and as columns the sources.\n",
    "    \"\"\"\n",
    "    \n",
    "    measured = list()\n",
    "    \n",
    "    for song in listdir(path + \"Sources/Test/\")[:n]:\n",
    "        tracks = list()\n",
    "\n",
    "        tracks.append(extractor(path + \"Sources/Test/\" + song + \"/bass.wav\", True)[0])\n",
    "        tracks.append(extractor(path + \"Sources/Test/\" + song + \"/drums.wav\", True)[0])\n",
    "        tracks.append(extractor(path + \"Sources/Test/\" + song + \"/other.wav\", True)[0])\n",
    "        tracks.append(extractor(path + \"Sources/Test/\" + song + \"/vocals.wav\", True)[0])\n",
    "\n",
    "        y_true = tf.stack(tracks,axis=3)\n",
    "        y_true = tf.concat(tf.unstack(y_true,axis=0),axis=0)\n",
    "\n",
    "        y_pred = tracks_predict(path + \"Mixtures/Test/\" + song + \"/mixture.wav\", net)[0]\n",
    "\n",
    "        measured+=measurer(y_true, y_pred)\n",
    "        \n",
    "    measured = tf.stack(measured)\n",
    "    \n",
    "    return tf.math.reduce_mean(measured,0), tf.math.reduce_std(measured,0), measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We compute the measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9240961, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-2.0841825, -0.6200529, -2.838591 , -0.6094992],\n",
       "       [ 4.1584187,  7.5702324,  3.5783527,  4.7879066],\n",
       "       [ 1.3268983,  1.3926016,  0.1526977,  2.8035827]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9240968, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[3.4224148, 3.4294777, 2.3956723, 2.6649582],\n",
       "       [5.3171062, 5.193449 , 2.9764698, 4.0756445],\n",
       "       [1.7441429, 2.2626538, 1.417179 , 1.9064392]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "measured_mean, measurerd_std, _ = comparer(50)\n",
    "display(measured_mean)\n",
    "display(measurerd_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We predict and save in a .wav file an audio track, to directly test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "out_vocals, sample_rate = tracks_predict(\"DSD100/Mixtures/Dev/051 - AM Contra - Heart Peripheral/mixture.wav\", net)\n",
    "tf.io.write_file('prediction/bass.wav', tf.audio.encode_wav(out_vocals[:,:,0], sample_rate = sample_rate))\n",
    "tf.io.write_file('prediction/drums.wav', tf.audio.encode_wav(out_vocals[:,:,1], sample_rate = sample_rate))\n",
    "tf.io.write_file('prediction/other.wav', tf.audio.encode_wav(out_vocals[:,:,2], sample_rate = sample_rate))\n",
    "tf.io.write_file('prediction/vocals.wav', tf.audio.encode_wav(out_vocals[:,:,3], sample_rate = sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Part 3: Experimental evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here we show all the result obtained during the work, even the one obtained with different configurations from the final one presented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First of all let us report here the metrics of the result obtained in the original paper, as reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|Measures||Bass|Drums|Other|Vocals|\n",
    "|------||------|------|------|------|\n",
    "|SDR||0.9$\\pm$2.7|2.4$\\pm$2|1.3$\\pm$2.4|0.8$\\pm$1.5|\n",
    "|SIR||4.6$\\pm$4.4|9.1$\\pm$4.3|7.2$\\pm$3.6|3.8$\\pm$4|\n",
    "|SAR||6.9$\\pm$2.3|7$\\pm$2.8|5.3$\\pm$2.9|2.8$\\pm$2.4|\n",
    "Original article results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For each experiment it is also provided the file with the pretrained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Experiment 1: Main network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here we show the result of the network presented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|Measures||Bass|Drums|Other|Vocals|\n",
    "|------||------|------|------|------|\n",
    "|SDR||-2.1$\\pm$3.4|-0.6$\\pm$3.4|-2.8$\\pm$2.4|-0.6$\\pm$2.7|\n",
    "|SIR||4.2$\\pm$5.3|7.6$\\pm$5.2|3.6$\\pm$3|4.8$\\pm$4|\n",
    "|SAR||1.3$\\pm$1.7|1.4$\\pm$2.3|0.1$\\pm$1.4|2.8$\\pm$1.9|\n",
    "Experiment 1 results: main network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can see that the results are worse, and in particular the SDR become negative. Analyzing the definition of the measures (see the paper from Vincent E. et al.) it results clear that the major problem is the introduction of artifact sounds, as the artifacts vector is the common element in the denominator of both the SDR and SAR measure.\n",
    "\n",
    "Listening to track predicted with this network confirm this deduction. It is possible to hear what the network was trying to extract, with some odd sounds added and sometimes some small contamination from other tracks.\n",
    "\n",
    "As said the result are far lower in quality than the one in the original article. Anyway, they are mostly still compatible with the original result, as considered the error in the estimations they often intersects.\n",
    "\n",
    "We deviated from the original work deciding to not include the fifth artificial track and to split the train in blocks for computational reason. It may be the major cause of the reduction in efficiency of our network.\n",
    "\n",
    "In particular, as we suspected that our different version of the train was the major cause of the problem, we also tried to increment the number of epochs, repetitions and to change the organizations of the blocks. All the experiment of this type lead to results almost identical to the one above, suggesting that we reached a stable point of our network.\n",
    "\n",
    "The extra track would have augmented the differenziation between tracks, we therefore tried different changes in the loss parameters and network parameters to compensate this effect. The result obtained were very similar to the Experiment 1 results, pointing probably at the fact that we reached the maximum possible performance for this architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Experiment 2: Double channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Everything we did up to now, as the original paper, was monoaural. That is: we used a single channel.\n",
    "\n",
    "But the database we used was composed by stereo audio track: with two channels. We wrote the code in such a way that the number of channel can be given and the network modified accordingly, therefore we tried it using both the channels.\n",
    "\n",
    "We trained the network on both channels, without chaingin any other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|Measures||Bass|Drums|Other|Vocals|\n",
    "|------||------|------|------|------|\n",
    "|SDR||-2$\\pm$3.4|-0.6$\\pm$3.6|-2.7$\\pm$2.3|-0.7$\\pm$2.6|\n",
    "|SIR||4.1$\\pm$5.2|7.8$\\pm$5.4|3.9$\\pm$3|5$\\pm$3.8|\n",
    "|SAR||1.4$\\pm$1.8|1.3$\\pm$2.4|0.1$\\pm$1.2|2.5$\\pm$2|\n",
    "Experiment 2 results: two channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As we can see the result are very similar to the one in the precedent experiment, leading us to belive that this type of network can easly be generalized to multi-channel audio track.\n",
    "\n",
    "The problem is the computational time: the stereo network took three times the monoaurual network to be trained."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
